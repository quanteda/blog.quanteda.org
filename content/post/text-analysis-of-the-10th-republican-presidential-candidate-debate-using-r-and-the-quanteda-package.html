---
title: "Text analysis of the 10th Republican Presidential candidate debate using R and the quanteda package"
author: "Kenneth Benoit and Gokhan Ciflikli"
date: "2019-02-12"
tags: ["blog"]
---



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>A frequent problem in processing texts is the need to segment one or few documents into many documents, based on segments that they contain marking units that the analyst will want to consider seperately.</p>
<p>This is a frequent feature of interview or debate <strong>transcripts</strong>, for instance, where a single long source document might contain numerous speech acts from individual speakers. For analysis, it’s likely that we would want to consider these speakers separately, perhaps with the ability to combine their speech acts later by spearker.</p>
<p>Here, we show how this can be done using the <a href="https://quanteda.io"><strong>quanteda</strong> R package</a>, using a debate from the 2016 U.S. presidential election campaign. This was the tenth debate among the Republican candidates for the 2016 election, took place in Houston, Texas on 25 February 2016, and was moderated by CNN. We demonstrate how to download, import, clean, parse by speaker, and analyze the debate by speaker.</p>
</div>
<div id="getting-the-text-into-r" class="section level2">
<h2>Getting the text into R</h2>
<p>The first step involves loading the debate into R. To read this text, we can use the <strong>rvest</strong> package, which will automatically extract text from the <a href="http://www.presidency.ucsb.edu/ws/index.php?pid=111634">US Presidency Project website</a>.</p>
<pre class="r"><code>library(&quot;rvest&quot;)
## Loading required package: xml2
scraping &lt;- read_html(&quot;http://www.presidency.ucsb.edu/ws/index.php?pid=111634&quot;)
txt &lt;- scraping %&gt;%
    html_nodes(&quot;p&quot;) %&gt;%
    html_text()</code></pre>
<p>We can see what the text looks like by examining the first and last parts of the text.</p>
<pre class="r"><code>head(txt)
## [1] &quot;About Search&quot;                                                                                                                                                                                                                                                    
## [2] &quot;&quot;                                                                                                                                                                                                                                                                
## [3] &quot;PARTICIPANTS:\nBen Carson;\nSenator Ted Cruz (TX);\nGovernor John Kasich (OH);\nSenator Marco Rubio (FL);\nDonald Trump;&quot;                                                                                                                                        
## [4] &quot;MODERATOR:\nWolf Blitzer (CNN); withPANELISTS:\nMaria Celeste Arrarás (Telemundo);\nDana Bash (CNN); and\nHugh Hewitt (Salem Radio Network)&quot;                                                                                                                     
## [5] &quot;BLITZER: We&#39;re live here at the University of Houston for the 10th Republican presidential debate. [applause]&quot;                                                                                                                                                   
## [6] &quot;An enthusiastic crowd is on hand here in the beautiful opera house at the Moore School of Music. Texas is the biggest prize next Tuesday, Super Tuesday, when 11 states vote, a day that will go a long way towards deciding who wins the Republican nomination.&quot;
tail(txt, 10)
##  [1] &quot;Nobody knows politicians better than I do. They&#39;re all talk, they&#39;re no action, nothing gets done. I&#39;ve watched it for years. Take a look at what&#39;s happening to our country.&quot;                                                                                                                                                                                                                                           
##  [2] &quot;All of the things that I&#39;ve been talking about, whether it&#39;s trade, whether it&#39;s building up our depleted military, whether it&#39;s taking care of our vets, whether it&#39;s getting rid of Common Core, which is a disaster, or knocking out Obamacare and coming up with something so much better, I will get it done. Politicians will never, ever get it done. And we will make America great again. Thank you. [applause]&quot;
##  [3] &quot;BLITZER: Mr. Trump, thank you.&quot;                                                                                                                                                                                                                                                                                                                                                                                          
##  [4] &quot;And thanks to each of the candidates, on behalf of everyone here at CNN and Telemundo. We also want to thank the Republican National Committee and the University of Houston. My thanks also to Hugh Hewitt, Maria Celeste, and Dana Bash.&quot;                                                                                                                                                                              
##  [5] &quot;Super Tuesday is only five days away.&quot;                                                                                                                                                                                                                                                                                                                                                                                   
##  [6] &quot;Presidential Candidate Debates, Republican Candidates Debate in Houston, Texas Online by Gerhard Peters and John T. Woolley, The American Presidency Project https://www.presidency.ucsb.edu/node/312591&quot;                                                                                                                                                                                                                
##  [7] &quot;The American Presidency ProjectJohn Woolley and Gerhard PetersContact&quot;                                                                                                                                                                                                                                                                                                                                                   
##  [8] &quot;Twitter Facebook&quot;                                                                                                                                                                                                                                                                                                                                                                                                        
##  [9] &quot;Copyright © The American Presidency ProjectTerms of Service | Privacy | Accessibility&quot;                                                                                                                                                                                                                                                                                                                                   
## [10] &quot;&quot;</code></pre>
</div>
<div id="cleaning-the-text" class="section level2">
<h2>Cleaning the text</h2>
<p>To get only the text spoken by each candidate, we still need to remove the non-text markers for events such as applause. We can do this using a substitution of the text we wish to remove for the null string "", using the powerful <strong>stringi</strong> package’s <code>stri_replace_all_regex()</code> function. (As an alternative, we could have used <code>gsub()</code> or <code>stringr::str_replace()</code> but we prefer <strong>stringi</strong>.)</p>
<pre class="r"><code>library(&quot;stringi&quot;, verbose = FALSE)
# removes interjections
txt &lt;- stri_replace_all_regex(txt, 
                              &quot;\\s*\\[(APPLAUSE|BELL RINGS|BELL RINGING|THE STAR-SPANGLED BANNER|COMMERCIAL BREAK|CROSSTALK|inaudible|LAUGHTER|CHEERING)\\]\\s*&quot;, &quot;&quot;,
                              case_insensitive = TRUE)</code></pre>
<p>Now we can see that the in-text notes such as “<code>[applause]</code>” are removed.</p>
<pre class="r"><code>tail(txt, 11)[1:3]
## [1] &quot;TRUMP: Thank you.&quot;                                                                                                                                                                                                                                                                                                                                                                                            
## [2] &quot;Nobody knows politicians better than I do. They&#39;re all talk, they&#39;re no action, nothing gets done. I&#39;ve watched it for years. Take a look at what&#39;s happening to our country.&quot;                                                                                                                                                                                                                                
## [3] &quot;All of the things that I&#39;ve been talking about, whether it&#39;s trade, whether it&#39;s building up our depleted military, whether it&#39;s taking care of our vets, whether it&#39;s getting rid of Common Core, which is a disaster, or knocking out Obamacare and coming up with something so much better, I will get it done. Politicians will never, ever get it done. And we will make America great again. Thank you.&quot;</code></pre>
</div>
<div id="creating-and-segmenting-the-corpus" class="section level2">
<h2>Creating and segmenting the corpus</h2>
<p>Let’s now put this into a single document, to create as a <strong>quanteda</strong> corpus object for processing and analysis.</p>
<pre class="r"><code>library(&quot;quanteda&quot;, warn.conflicts = FALSE, quietly = TRUE)
## Package version: 1.4.2
## Parallel computing: 2 of 12 threads used.
## See https://quanteda.io for tutorials and examples.</code></pre>
<p>Because we want this as a single document, we will combine all of the lines read as separate elements of our character vector <code>txt</code> into a single element. This creates a single “document” from the debate transcript. (Had we not scraped the document, for instance, it’s quite possible that we would have input it as a single document.)</p>
<pre class="r"><code>corp &lt;- corpus(paste(txt, collapse = &quot;\n&quot;), 
               docnames = &quot;presdebate-2016-02-25&quot;,
               metacorpus = list(
                   source = &quot;http://www.presidency.ucsb.edu/ws/index.php?pid=111634&quot;,
                   notes = &quot;10th Republican candidate debate, Houston TX 2016-02-25&quot;)
               )</code></pre>
<p>We can now use the <code>summary()</code> method for a corpus object to see a bit of information about the corpus we have just created.</p>
<pre class="r"><code>summary(corp)
## Corpus consisting of 1 document:
## 
##                   Text Types Tokens Sentences
##  presdebate-2016-02-25  3051  29978      2002
## 
## Source: http://www.presidency.ucsb.edu/ws/index.php?pid=111634
## Created: Wed Mar  6 10:46:41 2019
## Notes: 10th Republican candidate debate, Houston TX 2016-02-25</code></pre>
<p>Our goal in order to analyze this by speaker, is to redefine the corpus as a set of documents defined as a single speech acts, with a document variable identifying the speaker. We accomplish this through the <code>corpus_segment()</code> method, using the fact that each speaker is identified by a regular pattern such as “TRUMP:” or “BLITZER:”. These always start on a new line, and the colon (“:”) is always followed by a space. We can turn this into a <a href="https://www.regular-expressions.info/">regular expression</a>, and feed it as the <code>pattern</code> argument to the function <code>corpus_segment()</code>.</p>
<pre class="r"><code>corpseg &lt;- corpus_segment(corp, pattern = &quot;\\s*[[:upper:]]+:\\s+&quot;, 
                          valuetype = &quot;regex&quot;, case_insensitive = FALSE)</code></pre>
<p>We needed to add <code>case_insensitive = FALSE</code> because otherwise, the upper case character class will be overwritten, and we will pick up matches for things like the “now:” in “I’m quoting you now: Let me be…”.</p>
<p>This converts our single document into a corpus of 538 documents, extracting the regular expression. match in the text to <code>pattern</code></p>
<pre class="r"><code>summary(corpseg, 10)
## Corpus consisting of 533 documents, showing 10 documents:
## 
##                      Text Types Tokens Sentences             pattern
##   presdebate-2016-02-25.1    18     27         1 \n\nPARTICIPANTS:\n
##   presdebate-2016-02-25.2     7      7         1      \nMODERATOR:\n
##   presdebate-2016-02-25.3    16     21         1        PANELISTS:\n
##   presdebate-2016-02-25.4   165    287        20         \nBLITZER: 
##   presdebate-2016-02-25.5    21     24         1         \nBLITZER: 
##   presdebate-2016-02-25.6    24     28         3         \nBLITZER: 
##   presdebate-2016-02-25.7   114    198        13       \n\nBLITZER: 
##   presdebate-2016-02-25.8    68     92         5          \nCARSON: 
##   presdebate-2016-02-25.9     3      3         1         \nBLITZER: 
##  presdebate-2016-02-25.10    98    164        10          \nKASICH: 
## 
## Source: http://www.presidency.ucsb.edu/ws/index.php?pid=111634
## Created: Wed Mar  6 10:46:42 2019
## Notes: corpus_segment.corpus(corp, pattern = &quot;\\s*[[:upper:]]+:\\s+&quot;, valuetype = &quot;regex&quot;, case_insensitive = FALSE)</code></pre>
<p>Let’s rename <code>pattern</code> to something more descriptive, such as <code>speaker</code>. To do this robustly, we will lookup the position of the names of the docvars and replace it.</p>
<pre class="r"><code>names(docvars(corpseg))[which(names(docvars(corpseg)) == &quot;pattern&quot;)] &lt;- &quot;speaker&quot;</code></pre>
<p>We can clean up the patterns further through some replacements.</p>
<pre class="r"><code>docvars(corpseg, &quot;speaker&quot;) &lt;- stri_trim_both(docvars(corpseg, &quot;speaker&quot;))
docvars(corpseg, &quot;speaker&quot;) &lt;- stri_replace_all_fixed(docvars(corpseg, &quot;speaker&quot;), &quot;:&quot;, &quot;&quot;)
# a misspelling in the transcript
docvars(corpseg, &quot;speaker&quot;) &lt;- stri_replace_all_fixed(docvars(corpseg, &quot;speaker&quot;), &quot;ARRARAS&quot;, &quot;ARRASAS&quot;)</code></pre>
<p>Now we can see that the tags are better:</p>
<pre class="r"><code>table(docvars(corpseg, &quot;speaker&quot;))
## 
##      ARRARÁS         BASH      BLITZER       CARSON         CRUZ 
##           25           28          103           14           67 
##       HEWITT       KASICH    MODERATOR    PANELISTS PARTICIPANTS 
##           24           25            1            1            1 
##        RUBIO        TRUMP 
##           92          152</code></pre>
<p>We are only interested in the speakers who are presidential candidates, so let’s remove the moderator Wolf Blitzer, the panelists Dana Bash, Maria Celeste Arrarás, and Hugh Hewitt, and the generic tags “Moderator”, “Participants”, and “Panelists”.</p>
<pre class="r"><code>corpcand &lt;- corpus_subset(corpseg, !(speaker %in% c(&quot;ARRARÁS&quot;, &quot;BASH&quot;, &quot;BLITZER&quot;, &quot;HEWITT&quot;,
                                                    &quot;MODERATOR&quot;, &quot;PANELISTS&quot;, &quot;PARTICIPANTS&quot;)))</code></pre>
<p>Now we have only statements from the five Republican candidates in our corpus.</p>
<pre class="r"><code>corpcand
## Corpus consisting of 350 documents and 1 docvar.
unique(docvars(corpcand, &quot;speaker&quot;))
## [1] &quot;CARSON&quot; &quot;KASICH&quot; &quot;RUBIO&quot;  &quot;CRUZ&quot;   &quot;TRUMP&quot;</code></pre>
<p>Removing the final speaker (Blitzer) also removed some footer text that was picked up as following this tag.</p>
<pre class="r"><code>tail(docvars(corpseg, &quot;speaker&quot;), 1)
## [1] &quot;BLITZER&quot;
texts(corpseg)[ndoc(corpseg)] %&gt;%
    cat()
## Mr. Trump, thank you.
## And thanks to each of the candidates, on behalf of everyone here at CNN and Telemundo. We also want to thank the Republican National Committee and the University of Houston. My thanks also to Hugh Hewitt, Maria Celeste, and Dana Bash.
## Super Tuesday is only five days away.
## Presidential Candidate Debates, Republican Candidates Debate in Houston, Texas Online by Gerhard Peters and John T. Woolley, The American Presidency Project https://www.presidency.ucsb.edu/node/312591
## The American Presidency ProjectJohn Woolley and Gerhard PetersContact
## Twitter Facebook
## Copyright © The American Presidency ProjectTerms of Service | Privacy | Accessibility</code></pre>
<p>Because we removed Blitzer above when we created <code>corpcand</code>, we don’t need to worry about removing the Footer text identifying the source of the document as being the American Presidency Project.</p>
<pre class="r"><code>any(stringi::stri_detect_fixed(texts(corpcand), &quot;American Presidency Project&quot;))
## [1] FALSE</code></pre>
</div>
<div id="analysis-who-spoke-the-most" class="section level2">
<h2>Analysis: Who spoke the most?</h2>
<p>We can answer this question in two ways: by the greatest number of speech acts, created when a candidate spoke, and by the total number of words that a candidate spoke in the debate.</p>
<p>To count and compare speech acts, we can tabulate the speech acts and plot the speaker frequency as a barplot.</p>
<pre class="r"><code>par(mar = c(5, 6, .5, 1))
table(docvars(corpcand, &quot;speaker&quot;)) %&gt;%
    sort() %&gt;%
    barplot(horiz = TRUE, las = 1, xlab = &quot;Total Times Speaking&quot;)</code></pre>
<p><img src="/post/text-analysis-of-the-10th-republican-presidential-candidate-debate-using-r-and-the-quanteda-package_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>To compare candidates in terms of the total words spoke, we we can get the individual words from <code>ntoken()</code>.</p>
<pre class="r"><code>par(mar = c(5, 6, .5, 1))
texts(corpcand, groups = &quot;speaker&quot;) %&gt;%
    ntoken(remove_punct = TRUE) %&gt;%
    sort() %&gt;%
    barplot(horiz = TRUE, las = 1, xlab = &quot;Total Words Spoken&quot;)</code></pre>
<p><img src="/post/text-analysis-of-the-10th-republican-presidential-candidate-debate-using-r-and-the-quanteda-package_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>The <code>ntoken()</code> function does the work here of counting the tokens in the vector of texts returned by the call to <code>texts(corpcand, groups = "speaker")</code>, which extracts the texts from our segmented corpus and concatenates all texts by speaker. This results in a vector of the same 6 speakers as in our tabulation above. Passing through the <code>remove_punct = TRUE</code> option in the <code>ntoken()</code> call sends this argument through to <code>tokens()</code>, meaning we will not count punctutation characters as tokens.</p>
<p>In both examples, we can see that Trump spoke the most and Carson the least.</p>
</div>
<div id="analysis-what-were-they-saying" class="section level2">
<h2>Analysis: What were they saying?</h2>
<p>If we wanted to go further, we convert the segmented corpus into a <em>document-feature matrix</em> and apply one of many available psychological dictionaries to analyze the tone of each candidate’s remarks.</p>
<p>Here we demonstrate this using the Regressive Imagery Dictionary, from Martindale, C. (1975) <em>Romantic Progression: The Psychology of Literary History.</em> Washington, D.C.: Hemisphere. The code below automatically downloads a version of this dictionary in a format prepared for the WordStat software by <a href="http://www.provalisresearch.com">Provalis</a>, available from <a href="http://www.provalisresearch.com/Download/RID.ZIP" class="uri">http://www.provalisresearch.com/Download/RID.ZIP</a>. <strong>quanteda</strong> can import dictionaries formatted for WordStat, using the <code>dictionary()</code> function. We will apply the RID dictionary to find out who used what degree of “glory”-oriented language. (You might be able to guess the results already.)</p>
<pre class="r"><code># get the RID from the Provalis website
download.file(&quot;http://provalisresearch.com/Download/RID.ZIP&quot;, &quot;RID.zip&quot;)
unzip(&quot;RID.zip&quot;)
data_dictionary_RID &lt;- dictionary(file = &quot;RID.CAT&quot;, format = &quot;wordstat&quot;)
invisible(file.remove(&quot;RID.zip&quot;, &quot;RID.CAT&quot;, &quot;RID.exc&quot;))</code></pre>
<p>This is a nested dictionary object with three primary keys:</p>
<pre class="r"><code>names(data_dictionary_RID)
## [1] &quot;PRIMARY&quot;   &quot;SECONDARY&quot; &quot;EMOTIONS&quot;</code></pre>
<p>There are additional keys nested inside each of these. We can show the number of values for each nested entry for the “EMOTIONS” top-level key.</p>
<pre class="r"><code>lengths(data_dictionary_RID[[&quot;EMOTIONS&quot;]])
## POSITIVE_AFFECT         ANXIETY         SADNESS       AFFECTION 
##              70              49              75              65 
##      AGGRESSION  EXPRESSIVE_BEH           GLORY 
##             222              52              76</code></pre>
<p>We can inspect the category we will use (“Glory”) by looking at the last sub-key of the third key, “Emotions”.</p>
<pre class="r"><code>tail(data_dictionary_RID[[&quot;EMOTIONS&quot;]], 1)
## Dictionary object with 1 key entry.
## - [GLORY]:
##   - admir*, admirabl*, adventur*, applaud*, applaus*, arroganc*, arrogant*, audacity*, awe*, boast*, boastful*, brillianc*, brilliant*, caesar*, castl*, conque*, crown*, dazzl*, eagl*, elit*, emperor*, empir*, exalt*, exhibit*, exquisit*, extraordinary*, extrem*, fame, famed, famou*, foremost*, geniu*, glor*, gold*, golden*, grandeur*, great*, haughty*, hero*, homag*, illustriou*, kingdom*, magestic*, magnificent*, majestic*, majesty*, nobl*, outstand*, palac*, pomp*, prestig*, prid*, princ*, proud*, renown*, resplendent*, rich*, royal*, royalty*, sceptr*, scorn*, splendid*, splendor*, strut*, sublim*, superior*, superiority*, suprem*, thron*, triump*, victor*, victoriou*, victory*, wealth*, wonder*, wonderful*</code></pre>
<p>Let’s create a document-feature matrix from the candidate corpus, grouping the documents by speaker. This takes all of the speech acts and combines them by the value of “speaker”, so that the new number of documents is just five (one for each candidate).</p>
<pre class="r"><code>dfmatcand &lt;- dfm(corpcand, groups = &quot;speaker&quot;, verbose = TRUE)
## Creating a dfm from a corpus input...
##    ... lowercasing
##    ... found 350 documents, 2,507 features
##    ... grouping texts
##    ... created a 5 x 2,507 sparse dfm
##    ... complete. 
## Elapsed time: 0.08 seconds.</code></pre>
<p>Because the texts are of different lengths, we want to normalize them (by converting the feature counts into vectors of relative frequencies within document):</p>
<pre class="r"><code>dfmatcand &lt;- dfm_weight(dfmatcand, &quot;prop&quot;)</code></pre>
<p>Now we are in a position to apply the RID to the dfm, which matches on the “glob” formatted wildcard expressions that form the values of the RID in our <code>data_dictionary_RID</code> object.</p>
<pre class="r"><code>dfmatcandRID &lt;- dfm_lookup(dfmatcand, dictionary = data_dictionary_RID)</code></pre>
<p>Inspecting this, we see that all tokens have been matched to entries in the Regressive Imagery Dictionary, so that the new features are now “keys”, or dictionary categories, from the RID.</p>
<pre class="r"><code>head(dfmatcandRID, nf = 4)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">document</th>
<th align="right">PRIMARY.NEED.ORALITY</th>
<th align="right">PRIMARY.NEED.ANALITY</th>
<th align="right">PRIMARY.NEED.SEX</th>
<th align="right">PRIMARY.SENSATION.TOUCH</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">CARSON</td>
<td align="right">0.0024295</td>
<td align="right">0.0000000</td>
<td align="right">0</td>
<td align="right">0.0004859</td>
</tr>
<tr class="even">
<td align="left">CRUZ</td>
<td align="right">0.0012469</td>
<td align="right">0.0000000</td>
<td align="right">0</td>
<td align="right">0.0004156</td>
</tr>
<tr class="odd">
<td align="left">KASICH</td>
<td align="right">0.0004850</td>
<td align="right">0.0009699</td>
<td align="right">0</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="left">RUBIO</td>
<td align="right">0.0019881</td>
<td align="right">0.0000000</td>
<td align="right">0</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">TRUMP</td>
<td align="right">0.0007627</td>
<td align="right">0.0006356</td>
<td align="right">0</td>
<td align="right">0.0003813</td>
</tr>
</tbody>
</table>
<p>We can inspect the most common ones using the <code>topfeatures()</code> command, which here we will multiply by 100 to get slightly easier to interpret percentages.</p>
<pre class="r"><code>topfeatures(dfmatcandRID * 100, n = 10) %&gt;%
    round(2) %&gt;%
    knitr::kable(col.names = &quot;Percent&quot;)</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">Percent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SECONDARY.ABSTRACT_TOUGHT</td>
<td align="right">14.51</td>
</tr>
<tr class="even">
<td>SECONDARY.SOCIAL_BEHAVIOR</td>
<td align="right">11.79</td>
</tr>
<tr class="odd">
<td>SECONDARY.TEMPORAL_REPERE</td>
<td align="right">11.20</td>
</tr>
<tr class="even">
<td>SECONDARY.INSTRU_BEHAVIOR</td>
<td align="right">10.33</td>
</tr>
<tr class="odd">
<td>PRIMARY.REGR_KNOL.CONCRETENESS</td>
<td align="right">9.59</td>
</tr>
<tr class="even">
<td>SECONDARY.MORAL_IMPERATIVE</td>
<td align="right">3.72</td>
</tr>
<tr class="odd">
<td>EMOTIONS.AGGRESSION</td>
<td align="right">3.40</td>
</tr>
<tr class="even">
<td>PRIMARY.SENSATION.VISION</td>
<td align="right">2.54</td>
</tr>
<tr class="odd">
<td>EMOTIONS.AFFECTION</td>
<td align="right">2.05</td>
</tr>
<tr class="even">
<td>SECONDARY.RESTRAINT</td>
<td align="right">2.02</td>
</tr>
</tbody>
</table>
<p>We could probably spend a whole day analyzing this information, but here, let’s simply compare candidates on their relative use of language in the “Emotions: Glory” category of the RID. We do this by slicing out the feature with this label.</p>
<pre class="r"><code>dfmatcandRID[, &quot;EMOTIONS.GLORY&quot;]
## Document-feature matrix of: 5 documents, 1 feature (0.0% sparse).
## 5 x 1 sparse Matrix of class &quot;dfm&quot;
##         features
## docs     EMOTIONS.GLORY
##   CARSON   0.0009718173
##   CRUZ     0.0027015794
##   KASICH   0.0016973812
##   RUBIO    0.0019880716
##   TRUMP    0.0045760773</code></pre>
<p>To make this a vector, we force it using <code>as.vector()</code>, as there is no <code>drop = TRUE</code> option for dfm indexing. We then reattach the document labels (the candidate names) to this vector as names. We can plot it using a dotplot, showing that Trump was by far the highest user of this type of language.</p>
<pre class="r"><code>glory &lt;- as.vector(dfmatcandRID[, &quot;EMOTIONS.GLORY&quot;])
names(glory) &lt;- docnames(dfmatcandRID)
dotchart(sort(glory), 
         xlab = &quot;RID \&quot;Glory\&quot; terms used as a proportion of all terms&quot;,
         pch = 19, xlim = c(0, .005))</code></pre>
<p><img src="/post/text-analysis-of-the-10th-republican-presidential-candidate-debate-using-r-and-the-quanteda-package_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
</div>
<div id="comparing-one-candidate-to-another" class="section level2">
<h2>Comparing one candidate to another</h2>
<p>Thus far, we have employed some fairly simple plotting functions from the <strong>graphics</strong> package. We can also draw on some of <strong>quanteda</strong>’s “textplot” functions designed to work directly with its special object classes.</p>
<p>For instance, we can easily determine which words were used mnost differentially by Trump versus Cruz using some “keyness” statistics and plots. Here we recreate the dfm, remove punctuation and English stopwords, group by speaker, subset by just Trump and Cruz, compute keyness statistics, and then plot the keyness statistics. We can do all of this in one set of piped operations.</p>
<pre class="r"><code>dfm(corpcand, remove_punct = TRUE) %&gt;%
    dfm_remove(stopwords(&quot;english&quot;)) %&gt;%
    dfm_group(groups = &quot;speaker&quot;) %&gt;%
    dfm_subset(speaker %in% c(&quot;TRUMP&quot;, &quot;CRUZ&quot;)) %&gt;%
    textstat_keyness(target = &quot;TRUMP&quot;) %&gt;%
    textplot_keyness()</code></pre>
<p><img src="/post/text-analysis-of-the-10th-republican-presidential-candidate-debate-using-r-and-the-quanteda-package_files/figure-html/unnamed-chunk-32-1.png" width="960" /></p>
</div>
