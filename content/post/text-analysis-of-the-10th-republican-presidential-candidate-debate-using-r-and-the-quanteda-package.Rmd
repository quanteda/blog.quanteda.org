---
title: "Text analysis of the 10th Republican Presidential candidate debate using R and the quanteda package"
author: "Kenneth Benoit and Gokhan Ciflikli"
date: "2016-02-26"
tags: ["blog"]
output: html_document
---

```{r setup, include=FALSE, eval = TRUE}
require(knitr)
opts_chunk$set(echo = TRUE, tidy = TRUE)
```

On 25 February 2016, the tenth debate among the Republican candidates for the 2016 Presidential election took place in Houston, Texas, moderated by CNN. In this demonstration of the [quanteda](https://github.com/kbenoit/quanteda) package, I will show how to download, import, clean, parse by speaker, and analyze the debate by speaker.

The first step involves loading the debate into R.  To read this text, we can use the **rvest** package, which will automatically extract text from the [US Presidency Project website](http://www.presidency.ucsb.edu/ws/index.php?pid=111634).

```{r read in pdf, eval = TRUE}
library("rvest")
scraping <- read_html("http://www.presidency.ucsb.edu/ws/index.php?pid=111634")
transcriptText <- scraping %>%
                    html_nodes("p") %>%
                      html_text()
```

To get only the text spoken by each candidate, we still need to remove the non-text markers for events such as applause. We can do this using a substitution of the text we wish to remove for the null string "", using `gsub()`.

```{r regex, eval = TRUE}
# removes interjections
transcriptText <- gsub("\\s*\\[(APPLAUSE|BELL RINGS|BELL RINGING|THE STAR-SPANGLED BANNER|COMMERCIAL BREAK|CROSSTALK|inaudible|LAUGHTER|CHEERING)\\]\\s*", "", ignore.case = TRUE, transcriptText)
```

```{r paste3, eval = TRUE}
cat(paste(substring(transcriptText, 1, 1000), substring(transcriptText, 5001, 5655), collapse = "\n"))
```

Now that we have this as one "document", we need to load it into the **quanteda** package for processing and analysis.

```{r paste4, eval = TRUE}
require(quanteda, warn.conflicts = FALSE, quietly = TRUE)
```

```{r corpus, eval = TRUE}
transcriptCorpus <- corpus(transcriptText, metacorpus = list(
  source = "http://www.presidency.ucsb.edu/ws/index.php?pid=111634",
  notes = "10th Republican candidate debate, Houston TX 2016-02-25"))
```

```{r corpus summary, eval = TRUE}
summary(transcriptCorpus)
```

Our goal in order to analyze this by speaker, is to redefine the corpus as a set of documents defined as a single speech acts, with a document variable identifying the speaker. We accomplish this through the `corpus_segment()` method:

```{r segment, eval = TRUE}
transcriptCorpus <- corpus_segment(transcriptCorpus, pattern = "\\s*[[:upper:]]+:\\s+", valuetype = "regex")
```

```{r corpus summary 2, eval = TRUE}
summary(transcriptCorpus, 10)
```

We can clean up the patterns:

```{r clean up tags, eval = TRUE}
docvars(transcriptCorpus, "pattern") <- stringi::stri_trim_both(docvars(transcriptCorpus, "pattern"))
docvars(transcriptCorpus, "pattern") <- gsub(":", "", docvars(transcriptCorpus, "pattern"))
docvars(transcriptCorpus, "pattern") <- gsub("ARRARAS", "ARRASAS", docvars(transcriptCorpus, "pattern"))
```

```{r subset, eval = TRUE}
transcriptCorpus <- corpus_subset(transcriptCorpus, !(pattern %in% c("MALE", "COOPER")))
```

```{r corpus summary 3, eval = TRUE}
summary(transcriptCorpus, 10)
```

```{r table docvars, eval = TRUE}
table(docvars(transcriptCorpus, "pattern"))
```

Now we can start to perfom some analysis on the text. Who spoke the most in the debate, in words?

```{r barplot, eval = TRUE}
par(mar = c(5, 6, .5, 1))
barplot(sort(ntoken(texts(transcriptCorpus, groups = "pattern"), removePunct = TRUE)), horiz = TRUE, las = 1, xlab = "Total Words Spoken")
```

The `ntoken()` function does the work here of counting the tokens in the vector of texts returned by the call to `transcriptCorpus, groups = "pattern"`, which extracts the texts from our segmented corpus and concatenates all texts by speaker. This results in a vector of the same 10 speakers as in our tabulation above. Passing through the `removePunct = TRUE` option in the `ntoken()` call sends this argument through to `tokenize()`, meaning we will not count punctutation characters as tokens. (See `?quanteda::tokenize` for details.)

If we wanted to go further, we convert the segmented corpus into a _document-feature matrix_ and apply one of many available psychological dictionaries to analyze the tone of each candidate’s remarks. Here I will demonstrate using the Regressive Imagery Dictionary, from Martindale, C. (1975) _Romantic progression: The psychology of literary history._ Washington, D.C.: Hemisphere. The code below automatically downloads a version of this dictionary in a format prepared for the WordStat software by Provalis, available from http://www.provalisresearch.com/Download/RID.ZIP. **quanteda** can import dictionaries formatted for WordStat, using the `dictionary()` function.

Here, we will apply the RID dictionary to find out who used what degree of “glory”-oriented language. (You might be able to guess the results already.)

```{r dictionary, eval = TRUE}
# get the RID from the Provalis website
RIDzipfile <- download.file("http://provalisresearch.com/Download/RID.ZIP", "RID.zip")
unzip("RID.zip")
data_dictionary_RID <- dictionary(file = "RID.CAT", format = "wordstat")
file.remove("RID.zip", "RID.CAT", "RID.exc")
```

This is a nested dictionary object with three primary keys:
```{r dictionary names, eval = TRUE}
names(data_dictionary_RID)
```

We can inspect the category we will use ("Glory") by looking at the last sub-key of the third key, "Emotions".
```{r dictionary tail, eval = TRUE}
tail(data_dictionary_RID[["EMOTIONS"]], 1)
```

Now we will extract just the candidates, using the `subset()` method for a corpus class object, and then create a document-feature matrix from this corpus, grouping the documents by speaker as we did before.

```{r dfm, eval = TRUE}
transcriptCorpusCands <- corpus_subset(transcriptCorpus, pattern %in% c("TRUMP", "CRUZ", "RUBIO", "KASICH", "CARSON"))
canddfm <- dfm(transcriptCorpusCands, groups = "pattern")
```

Because the texts are of different lengths, we want to normalize them (by converting the feature counts into vectors of relative frequencies within document):

```{r weight, eval = TRUE}
canddfmRel <- dfm_weight(canddfm, "prop")
```

Now we are in a position to apply the RID to the dfm, which matches on the “glob” formatted wildcard expressions that form the values of the RID in our `data_dictionary_RID` object.

```{r lookup, eval = TRUE}
canddfmRelRID <- dfm_lookup(canddfmRel, data_dictionary_RID)
head(canddfmRelRID)
```

```{r features, eval = TRUE}
topfeatures(canddfmRelRID, n = 20)
```

We could probably spend a whole day analyzing this information, but here, let’s simply compare candidates on their relative use of language in the “Emotions: Glory” category of the RID. We do this by slicing out the feature with this label, converting this to a vector (as there is no ```drop = TRUE``` option for dfm indexing), and then reattaching the document labels to this vector so that it will be named vector. We then send it to the ```dotchart()``` for a simple plot, showing that Trump was by far the highest user of this type of language.

```{r emotions, eval = TRUE}
canddfmRelRID[, "EMOTIONS.GLORY"]
```

```{r dotchart, eval = TRUE}
glory <- as.vector(canddfmRelRID[, "EMOTIONS.GLORY"])
names(glory) <- docnames(canddfmRelRID)
dotchart(sort(glory), xlab = "RID \"Glory\" terms used as a proportion of all terms",
         pch = 19, xlim = c(0, .005))
```

