---
title: "Announcing the quanteda v2 release"
author: "Kenneth Benoit and Kohei Watanabe"
date: '2020-02-27'
tags: ["blog"]
blogdown::html_page:
  highlight: tango
---

```{r, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE)
```

Following over a year of planning, discusion, design, and -- most significantly -- programming and testing, we are proud to announce the version 2 release of the **quanteda** package.  

**quanteda** 2.0 is a major update introducing some major changes and new features, detailed here.

## What's new in v2.0

1.  New corpus object structure.  

    The internals of the corpus object have been redesigned, and now are based around a character vector with meta- and system-data in attributes.  Docvars are now handled separately from the texts, in the same way that docvars are handled for tokens objects.
    
    ```{r}
    library(quanteda, warn.conflicts = FALSE)
    is.character(data_corpus_inaugural)
    ```
    
    These are all updated to work with the existing extractor and replacement functions.  Your old corpus ojbects are automatically upgraded to the 2.0 structure by **quanteda**'s functions but you can also do it manually using `as.corpus()`. 
    
    This means that all functions that operate on character vectors will also operate on a corpus, although it might be necessary to drop the special attributes using `as.character()`.  This is how `spacyr::spacy_parse()` for instance now works with a **quanteda** corpus, by using the fact that it's just a souped-up, specially classed character vector.
    
    One of the biggest headaches we faced in making the structural change was that other package authors -- including the (as of today) 20 packages that have strong reverse dependencies on **quanteda** had written functions that bypassed the accessor and replacement functions, despite our explicit plea not to do this in the documentation.  By working with these package authors, however, we were able to correct almost all of this.
    
2.  New metadata handling.

    Corpus-level metadata is now inserted in a user metadata list via `meta()` and `meta<-()`.  `metacorpus()` is kept as a synonym for `meta()`, for backwards compatibility.  Additional system-level corpus information is also recorded, but automatically when an object is created.  
    
    Document-level metadata is deprecated, and now all document-level information is simply a "docvar".  For backward compatibility, `metadoc()` is kept and will insert document variables (docvars) with the name prefixed by an underscore.
    
3.  Corpus objects now store default summary statistics for efficiency.  When these are present, `summary.corpus()` retrieves them rather than computing them on the fly.

4.  New index operators have been defined for core objects.  The main change here is to redefine the `$` operator for corpus, tokens, and dfm objects (all objects that retain docvars) to allow this operator to access single docvars by name.  Some other index operators have been redefined as well, such as `[.corpus` returning a slice of a corpus, and `[[.corpus` returning the texts from a corpus.

    ```{r}
    # view a docvar
    tail(data_corpus_inaugural$Party)
    
    # or reassign one
    data_corpus_inaugural$Party <- "hidden"
    head(docvars(data_corpus_inaugural), 4)
    
    # or delete one
    data_corpus_inaugural$Party <- NULL
    tail(docvars(data_corpus_inaugural), 4)
    ```

    See the full details at https://github.com/quanteda/quanteda/wiki/indexing_core_objects.
    
5.  `*_subset()` functions.  

     The `subset` argument now must be logical, and the `select` argument has been removed.  (This is part of `base::subset()` but has never made sense, either in **quanteda** or **base**.)

6.  Return format from `textstat_simil()` and `textstat_dist()`.

    Now defaults to a sparse matrix from the **Matrix** package, but coercion methods are provided for `as.data.frame()`, to make these functions return a data.frame just like the other textstat functions.  With the conversion to `data.frame`, all documents (or features) are returned as pairwise sets.
    
    Additional coercion methods are provided for `as.dist()`, `as.simil()`, and `as.matrix()`.
    
    Example:
    ```{r}
    dfmat <- corpus_subset(data_corpus_inaugural, Year > 1990) %>%
        dfm(remove_punct = TRUE)
    tstat <- textstat_simil(dfmat, method = "cosine")
    tstat
    ```
    
    This now easily becomes a data.frame for "tidy"ing:
    ```{r}
    # see the top most similar speeches
    as.data.frame(tstat) %>% 
        dplyr::arrange(-cosine)
    ```
    Or a list, similar to `tm::findAssocs()`:
    ```{r}
    textstat_simil(dfmat, margin = "features", method = "cosine") %>%
        as.list(n = 6) %>%
        head()
    ```
    
7.  The settings functions (and related slots and object attributes) are gone.  These are now replaced by a new `meta(x, type = "object")` that records object-specific meta-data, including settings such as the `n` for tokens (to record the `ngrams`).

    Under the hood, this enabled us to design a vessel to contain three types of medata: user metadata, which can be controlled entirely by the user and can contain anything (since it's a list), and is accessed by `meta()` and `meta()<-`; system metadata which is set when an object is created; and object metadata, which is automatically set when an object is modified or created, and includes options that the user controls.
    
    Example:
    ```{r}
    corp <- corpus("This is my one-document corpus.")
    meta(corp) <- list(source = "My blog post.",
                       mymatrix = matrix(1:6, nrow = 2))
    meta(corp)
    
    meta(corp, type = "system")
    ```
    

8.  All included data objects are upgraded to the new formats.  This includes the corpus objects, the single dfm data object, and the LSD 2015 dictionary object.

9.  New print methods for core objects (corpus, tokens, dfm, dictionary) now exist, each with new global options to control the number of documents shown, as well as the length of a text snippet (corpus), the tokens (tokens), dfm cells (dfm), or keys and values (dictionary).  Similar to the extended printing options for dfm objects, printing of corpus objects now allows for brief summaries of the texts to be printed, and for the number of documents and the length of the previews to be controlled by new global options.

    The printing of objects is smarter and provides more user control, but looks better by default.  Examples:
    
    ```{r}
    data_corpus_inaugural
    print(data_corpus_inaugural, max_ndoc = 1, max_nchar = 20)
    
    print(tokens(data_corpus_inaugural), max_ndoc = 3, max_ntok = 6)
    
    dfm(data_corpus_inaugural)
    ```
    
    The defaults for printing are all user-controllable via `quanteda_options()` as global settings, including whether to print summary information, or whether to print any section of a preview object at all.  For instance:
    
    ```{r}
    quanteda_options(print_tokens_max_ndoc = 0)
    tokens(data_corpus_inaugural)
    ```

10.  All textmodels and related functions have been moved to a new package **quanteda.textmodels**.  This makes them easier to maintain and update, and keeps the size of the core package down.

    This new package also includes the data objects that are primarily used as examples for scaling (which will be removed from **quanteda** in the next minor update), and also includes some new data objects as well.  See more about this package at https://github.com/quanteda/quanteda.textmodels.
    
11. **quanteda** v2 implements major changes to the `tokens()` constructor.  These are designed to simplify the code and its maintenance in **quanteda**, to allow users to work with other (external) tokenizers, and to improve consistency across the tokens processing options.  Changes include:

    -  A new method `tokens.list(x, ...)` constructs a `tokens` object from named list of characters, allowing users to tokenize texts using some other function (or package) such as `tokenize_words()`, `tokenize_sentences()`, or `tokenize_tweets()` from the **tokenizers** package, or the list returned by `spacyr::spacy_tokenize()`.  This allows users to use their choice of tokenizer, as long as it returns a named list of characters.  With `tokens.list()`, all tokens processing (`remove_*`) options can be applied, or the list can be converted directly to a `tokens` object without processing using `as.tokens.list()`.  For instance:
    
    ```{r}
    txt <- c(doc1 = "I live in the United Kingdom.", 
             doc2 = "Excellent document two.")
    tokenizers::tokenize_words(txt) %>%
        tokens()
    ```
    as well as:
    ```{r}
    spacyr::spacy_parse(txt) %>%
        spacyr::entity_consolidate() %>%
        as.tokens(include_pos = "pos")
    ```
    
    - All tokens options are now _intervention_ options, to split or remove things that by default are not split or removed.  All `remove_*` options to `tokens()` now remove them from tokens objects by calling `tokens.tokens()`, after constructing the object.  "Pre-processing" is now  actually post-processing using `tokens_*()` methods internally, after a conservative tokenization on token boundaries. This both improves performance and improves consistency in handling special characters (e.g. Twitter characters) across different tokenizer engines.    
    
    Note that `tokens.tokens()` will remove what is found, but cannot "undo" a removal -- for instance it cannot replace missing punctuation characters if these have already been removed.

    - The option `remove_hyphens` is removed and deprecated, but replaced by `split_hyphens`.  This preserves infix (internal) hyphens rather than splitting them.  This behaviour is implemented in both the `what = "word"` and `what = "word2"` tokenizer options.  This option is `FALSE` by default.
    
    -  The option `remove_twitter` has been removed.  The new `what = "word"` is a smarter tokenizer that preserves social media tags, URLs, and email-addresses.  "Tags" are defined as valid social media hashtags and usernames (using Twitter rules for validity) rather than removing the `#` and `@` punctuation characters, even if `remove_punct = TRUE`.
    
    While we have made it easier than ever to use any tokenizer you wish, the default **quanteda** tokenizer is smarter than ever. By default, it preserves social media usernames and hashtags, preserves URLs, and keeps infix hyphens.  With the `remove_` options however, a user can change this at any time, even after having formed tokens using the (conservative) defaults. 
    

## New features

* We changed the default value of the `size` argument in `dfm_sample()` to the number of features, not the number of documents.
* We fixed a few CRAN-related issues (compiler warnings on Solaris and encoding warnings on r-devel-linux-x86_64-debian-clang.)
* Added `startpos` and `endpos` arguments to `tokens_select()`, for selecting on token positions relative to the start or end of the tokens in each document.
* We added a `convert()` method for corpus objects, to convert them into data.frame or json formats.
* We added a `spacy_tokenize()` method for corpus objects, to provide direct access via the **spacyr** package.

## Behaviour changes

* We added a `force = TRUE` option and error checking for the situations of applying `dfm_weight()` or `dfm_group()` to a dfm that has already been weighted.  The function `textstat_frequency()` now allows passing this argument to `dfm_group()` via `...`.
* `textstat_frequency()` now has a new argument for resolving ties when ranking term frequencies, defaulting to the "min" method.
* New docvars accessor and replacement functions are available for corpus, tokens, and dfm objects via `$`.  (See Index Operators for Core Objects above.)
* `textstat_entropy()` now produces a data.frame that is more consistent with other `textstat_*` methods.

## Bug fixes and stability enhancements

*  docnames are now enforced to be character (formerly, could be numeric for some objects).
*  docnames are now enforced to be strictly unique for all object classes.
*  Grouping operations in `tokens_group()` and `dfm_group()` are more robust to using multiple grouping variables, and preserve these correctly as docvars in the new dfm.
*  We made some fixes to documented ... objects in two functions that were previously causing CRAN check failures on the release of 1.5.2.

## Other improvements

* The included corpus objects have been cleaned up and augmented with improved meta-data and docvars.  The inaugural speech corpus, for instance, now includes the President's political party affiliation.
